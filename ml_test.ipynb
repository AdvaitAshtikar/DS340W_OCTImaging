{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N35tB2m9_WDx",
    "outputId": "a8bac9bf-ae38-4546-865a-6306180906a4"
   },
   "outputs": [],
   "source": [
    "# installing the necessary environment variables\n",
    "%pip install torch torchvision opencv-python albumentations efficientnet_pytorch kagglehub pandas scikit-learn simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8L23FA-X_WDy"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6VHM-ld4_WDz",
    "outputId": "9f7f3563-5b71-461d-c31d-defdd98d3a33"
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# PyTorch dataset for loading both G1020 and ORIGA data\n",
    "\n",
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, labels, transform=None, mask_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"Mask not found at: {self.mask_paths[idx]}\")\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Apply transformations to the mask\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        else:\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Get the label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, mask, label\n",
    "\n",
    "# Define the transformations for images and masks (same as before)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load G1020 dataset using kagglehub\n",
    "# https://www.kaggle.com/datasets/arnavjain1/glaucoma-datasets\n",
    "path = kagglehub.dataset_download(\"arnavjain1/glaucoma-datasets\")\n",
    "\n",
    "# Set up paths for G1020\n",
    "image_dir_g1020 = os.path.join(path, 'G1020', 'Images_Square')\n",
    "mask_dir_g1020 = os.path.join(path, 'G1020', 'Masks_Square')\n",
    "csv_file_g1020 = os.path.join(path, 'G1020', 'G1020.csv')\n",
    "\n",
    "# Load G1020 CSV\n",
    "df_g1020 = pd.read_csv(csv_file_g1020)\n",
    "image_paths_g1020 = [os.path.join(image_dir_g1020, img_name) for img_name in df_g1020['imageID']]\n",
    "mask_paths_g1020 = [os.path.join(mask_dir_g1020, img_name.replace('.jpg', '.png')) for img_name in df_g1020['imageID']]\n",
    "labels_g1020 = df_g1020['binaryLabels'].values\n",
    "\n",
    "# Load ORIGA dataset\n",
    "image_dir_origa = os.path.join(path, 'ORIGA', 'Images_Square')\n",
    "mask_dir_origa = os.path.join(path, 'ORIGA', 'Masks_Square')\n",
    "csv_file_origa = os.path.join(path, 'ORIGA', 'OrigaList.csv')\n",
    "\n",
    "# Load ORIGA CSV\n",
    "df_origa = pd.read_csv(csv_file_origa)\n",
    "image_paths_origa = [os.path.join(image_dir_origa, img_name) for img_name in df_origa['Filename']]\n",
    "mask_paths_origa = [os.path.join(mask_dir_origa, img_name.replace('.jpg', '.png')) for img_name in df_origa['Filename']]\n",
    "labels_origa = df_origa['Glaucoma'].values\n",
    "\n",
    "# Combine datasets\n",
    "image_paths = image_paths_g1020 + image_paths_origa\n",
    "mask_paths = mask_paths_g1020 + mask_paths_origa\n",
    "labels = np.concatenate((labels_g1020, labels_origa))\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = GlaucomaDataset(image_paths, mask_paths, labels, transform=image_transform, mask_transform=mask_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Testing: Load a batch of images, masks, and labels\n",
    "images, masks, labels = next(iter(dataloader))\n",
    "print(images.shape, masks.shape, labels.shape)  # Should print shapes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8e3cDnm_WD0"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoding\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoding\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec4 = self.conv_block(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = self.conv_block(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = self.conv_block(128, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(nn.MaxPool2d(2)(e1))\n",
    "        e3 = self.enc3(nn.MaxPool2d(2)(e2))\n",
    "        e4 = self.enc4(nn.MaxPool2d(2)(e3))\n",
    "\n",
    "        # Decoding\n",
    "        d4 = self.upconv4(e4)\n",
    "        d4 = torch.cat((d4, e3), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e2), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e1), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        return self.conv_last(d2)\n",
    "\n",
    "unet_model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8NhHjQ-_WD0",
    "outputId": "9eb54e37-7c05-40ae-b977-119e4babea0f"
   },
   "outputs": [],
   "source": [
    "efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "num_features = efficientnet_model._fc.in_features\n",
    "efficientnet_model._fc = nn.Linear(num_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tOnYlpcE_WD1",
    "outputId": "e153f505-5b27-4ef0-e244-e7dc16486904"
   },
   "outputs": [],
   "source": [
    "# Training Loop for U-Net (Segmentation)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet_model = unet_model.to(device)\n",
    "criterion_seg = nn.BCEWithLogitsLoss()  # Using BCEWithLogitsLoss, so no sigmoid in model\n",
    "optimizer_seg = optim.Adam(unet_model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(6):\n",
    "    unet_model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"Starting epoch {epoch + 1} for U-Net...\")\n",
    "\n",
    "    for batch_idx, (images, masks, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)  # Ensure masks are moved to the correct device\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = unet_model(images)\n",
    "        loss = criterion_seg(outputs, masks)  # Use masks for segmentation loss\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_seg.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_seg.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/10], Batch [{batch_idx + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {running_loss / len(dataloader):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EiWW3K2hdtIK",
    "outputId": "82d905fa-2709-4a10-bfbf-906a49b79ed2"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval().to(device)  # Ensure the model is on the right device\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 3:\n",
    "                images, masks, labels = batch  # Unpack images, masks, labels\n",
    "            elif len(batch) == 2:\n",
    "                images, labels = batch  # For cases with only images and labels\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()  # Convert to probabilities\n",
    "\n",
    "            # Convert probabilities to binary predictions (thresholding at 0.5)\n",
    "            binary_preds = (preds > 0.5).astype(int).flatten()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(binary_preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Run the accuracy evaluation\n",
    "evaluate_accuracy(efficientnet_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1S2pQKC_WD1"
   },
   "outputs": [],
   "source": [
    "# when using sigmoid model\n",
    "\n",
    "'''\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def evaluate_classification_model(model, dataloader):\n",
    "  model.to(device)\n",
    "  model.eval()\n",
    "  all_preds = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "      for images, labels in dataloader:\n",
    "          images = images.to(device)\n",
    "          labels = labels.to(device)\n",
    "          outputs = model(images)\n",
    "          preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "          all_preds.extend(preds)\n",
    "          all_labels.extend(labels.cpu().numpy())\n",
    "  auc = roc_auc_score(all_labels, all_preds)\n",
    "  print(f\"AUC-ROC: {auc}\")\n",
    "\n",
    "evaluate_classification_model(efficientnet_model, dataloader)\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8il85Ai2d23K",
    "outputId": "a8d71507-f165-43f5-85c1-d2278e37ce14"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import json\n",
    "\n",
    "refuge_json_path = os.path.join(path, 'REFUGE', 'val', 'index.json')\n",
    "image_dir_refuge = os.path.join(path, 'REFUGE', 'val', 'Images_Cropped')\n",
    "mask_dir_refuge = os.path.join(path, 'REFUGE', 'val', 'Masks_Cropped')\n",
    "\n",
    "# Load the JSON file containing the actual values\n",
    "with open(refuge_json_path) as f:\n",
    "    refuge_data = json.load(f)\n",
    "\n",
    "# Prepare image paths, mask paths, and labels\n",
    "image_paths_refuge = []\n",
    "mask_paths_refuge = []\n",
    "labels_refuge = []\n",
    "\n",
    "for key in refuge_data:\n",
    "    img_info = refuge_data[key]\n",
    "    img_name = img_info['ImgName']\n",
    "    label = img_info['Label']\n",
    "\n",
    "    # Construct the full paths\n",
    "    image_paths_refuge.append(os.path.join(image_dir_refuge, img_name))\n",
    "    mask_paths_refuge.append(os.path.join(mask_dir_refuge, img_name.replace('.jpg', '.png')))  # Assuming masks are PNGs\n",
    "    labels_refuge.append(label)\n",
    "\n",
    "# Convert labels to a numpy array\n",
    "labels_refuge = np.array(labels_refuge)\n",
    "\n",
    "# Create a DataLoader for the REFUGE dataset\n",
    "refuge_dataset = GlaucomaDataset(image_paths_refuge, mask_paths_refuge, labels_refuge, transform=image_transform, mask_transform=mask_transform)\n",
    "refuge_dataloader = DataLoader(refuge_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientnet_model.to(device)\n",
    "\n",
    "# Evaluate the REFUGE dataset\n",
    "evaluate_accuracy(efficientnet_model, refuge_dataloader)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, masks, labels in refuge_dataloader:\n",
    "    images = images.to(device)\n",
    "    outputs = efficientnet_model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=[\"No Glaucoma\", \"Glaucoma\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "id": "OcJDBh6t_WD2",
    "outputId": "9b33e1c1-7e7b-4c6d-bbc6-a6a0d1ac1765"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_cdr(disc_mask, cup_mask):\n",
    "\n",
    "    disc_contours, _ = cv2.findContours(disc_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(disc_contours) > 0:\n",
    "        disc_cnt = max(disc_contours, key=cv2.contourArea)\n",
    "        _, _, _, disc_height = cv2.boundingRect(disc_cnt)\n",
    "    else:\n",
    "        disc_height = 1  # Avoid division by zero\n",
    "\n",
    "    cup_contours, _ = cv2.findContours(cup_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cup_contours) > 0:\n",
    "        cup_cnt = max(cup_contours, key=cv2.contourArea)\n",
    "        _, _, _, cup_height = cv2.boundingRect(cup_cnt)\n",
    "    else:\n",
    "        cup_height = 0\n",
    "\n",
    "    cdr = cup_height / disc_height\n",
    "    return cdr\n",
    "unet_model.eval()\n",
    "cdr_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        start_time = time.time()\n",
    "        output_masks = unet_model(images)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            output_mask = output_masks[i].cpu().numpy().squeeze()\n",
    "            output_mask = (output_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "            # Assume disc_mask and cup_mask separation logic here\n",
    "            disc_mask = output_mask  # Modify this based on your actual model output\n",
    "            cup_mask = output_mask    # Modify this based on your actual model output\n",
    "\n",
    "            # Calculate CDR\n",
    "            cdr = calculate_cdr(disc_mask, cup_mask)\n",
    "            cdr_list.append(cdr)\n",
    "\n",
    "print(cdr_list)  # Output the CDRs for verification\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
