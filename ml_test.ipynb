{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EImPycqRGdpB",
    "outputId": "302884f2-cc89-4b6c-be6c-c82324173cab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/codespace/.python/current/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in /home/codespace/.python/current/lib/python3.12/site-packages (0.20.0)\n",
      "Requirement already satisfied: opencv-python in /home/codespace/.python/current/lib/python3.12/site-packages (4.10.0.84)\n",
      "Requirement already satisfied: albumentations in /home/codespace/.python/current/lib/python3.12/site-packages (1.4.20)\n",
      "Requirement already satisfied: efficientnet_pytorch in /home/codespace/.python/current/lib/python3.12/site-packages (0.7.1)\n",
      "Requirement already satisfied: kagglehub in /home/codespace/.python/current/lib/python3.12/site-packages (0.3.3)\n",
      "Requirement already satisfied: pandas in /home/codespace/.local/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: matplotlib in /home/codespace/.local/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: simplejson in /home/codespace/.python/current/lib/python3.12/site-packages (3.19.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.12/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/codespace/.python/current/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /home/codespace/.local/lib/python3.12/site-packages (from torchvision) (2.1.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/codespace/.local/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/codespace/.local/lib/python3.12/site-packages (from albumentations) (1.14.1)\n",
      "Requirement already satisfied: PyYAML in /home/codespace/.local/lib/python3.12/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from albumentations) (2.9.2)\n",
      "Requirement already satisfied: albucore==0.0.19 in /home/codespace/.python/current/lib/python3.12/site-packages (from albumentations) (0.0.19)\n",
      "Requirement already satisfied: eval-type-backport in /home/codespace/.python/current/lib/python3.12/site-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /home/codespace/.python/current/lib/python3.12/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from albucore==0.0.19->albumentations) (3.10.5)\n",
      "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/codespace/.python/current/lib/python3.12/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/codespace/.local/lib/python3.12/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/codespace/.python/current/lib/python3.12/site-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing the necessary environment variables\n",
    "%pip install torch torchvision opencv-python albumentations efficientnet_pytorch kagglehub pandas scikit-learn matplotlib simplejson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJp4-2msGdpD"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GKtbL2tMGdpE",
    "outputId": "4a598253-6c4e-4a85-aa0c-cf59a7830e87"
   },
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "# PyTorch dataset for loading both G1020 and ORIGA data\n",
    "\n",
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, labels, transform=None, mask_transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Load the corresponding mask\n",
    "        mask = cv2.imread(self.mask_paths[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"Mask not found at: {self.mask_paths[idx]}\")\n",
    "\n",
    "        # Apply transformations to the image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Apply transformations to the mask\n",
    "        if self.mask_transform:\n",
    "            mask = self.mask_transform(mask)\n",
    "        else:\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        # Get the label\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "\n",
    "        return image, mask, label\n",
    "\n",
    "# Define the transformations for images and masks (same as before)\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load G1020 dataset using kagglehub\n",
    "# https://www.kaggle.com/datasets/arnavjain1/glaucoma-datasets\n",
    "path = kagglehub.dataset_download(\"arnavjain1/glaucoma-datasets\")\n",
    "\n",
    "# Set up paths for G1020\n",
    "image_dir_g1020 = os.path.join(path, 'G1020', 'Images_Square')\n",
    "mask_dir_g1020 = os.path.join(path, 'G1020', 'Masks_Square')\n",
    "csv_file_g1020 = os.path.join(path, 'G1020', 'G1020.csv')\n",
    "\n",
    "# Load G1020 CSV\n",
    "df_g1020 = pd.read_csv(csv_file_g1020)\n",
    "image_paths_g1020 = [os.path.join(image_dir_g1020, img_name) for img_name in df_g1020['imageID']]\n",
    "mask_paths_g1020 = [os.path.join(mask_dir_g1020, img_name.replace('.jpg', '.png')) for img_name in df_g1020['imageID']]\n",
    "labels_g1020 = df_g1020['binaryLabels'].values\n",
    "\n",
    "# Load ORIGA dataset\n",
    "image_dir_origa = os.path.join(path, 'ORIGA', 'Images_Square')\n",
    "mask_dir_origa = os.path.join(path, 'ORIGA', 'Masks_Square')\n",
    "csv_file_origa = os.path.join(path, 'ORIGA', 'OrigaList.csv')\n",
    "\n",
    "# Load ORIGA CSV\n",
    "df_origa = pd.read_csv(csv_file_origa)\n",
    "image_paths_origa = [os.path.join(image_dir_origa, img_name) for img_name in df_origa['Filename']]\n",
    "mask_paths_origa = [os.path.join(mask_dir_origa, img_name.replace('.jpg', '.png')) for img_name in df_origa['Filename']]\n",
    "labels_origa = df_origa['Glaucoma'].values\n",
    "\n",
    "# Combine datasets\n",
    "image_paths = image_paths_g1020 + image_paths_origa\n",
    "mask_paths = mask_paths_g1020 + mask_paths_origa\n",
    "labels = np.concatenate((labels_g1020, labels_origa))\n",
    "\n",
    "# Create the dataset and dataloader\n",
    "dataset = GlaucomaDataset(image_paths, mask_paths, labels, transform=image_transform, mask_transform=mask_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Testing: Load a batch of images, masks, and labels\n",
    "images, masks, labels = next(iter(dataloader))\n",
    "print(images.shape, masks.shape, labels.shape)  # Should print shapes accordingly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISYpFLawGdpF"
   },
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoding\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        # Decoding\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec4 = self.conv_block(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = self.conv_block(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = self.conv_block(128, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoding\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(nn.MaxPool2d(2)(e1))\n",
    "        e3 = self.enc3(nn.MaxPool2d(2)(e2))\n",
    "        e4 = self.enc4(nn.MaxPool2d(2)(e3))\n",
    "\n",
    "        # Decoding\n",
    "        d4 = self.upconv4(e4)\n",
    "        d4 = torch.cat((d4, e3), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e2), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e1), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        return self.conv_last(d2)\n",
    "\n",
    "unet_model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veuyzOJRGdpG",
    "outputId": "925612af-7c12-4ee2-baec-c9ffedad2d51"
   },
   "outputs": [],
   "source": [
    "efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "num_features = efficientnet_model._fc.in_features\n",
    "efficientnet_model._fc = nn.Linear(num_features, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8rKmjwSjGdpG",
    "outputId": "5cd051b6-fe03-409b-ca4e-6e589ec945c5"
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Training Loop for U-Net (Segmentation)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet_model = unet_model.to(device)\n",
    "criterion_seg = nn.BCEWithLogitsLoss()  # Using BCEWithLogitsLoss, so no sigmoid in model\n",
    "optimizer_seg = optim.Adam(unet_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Using GradScaler for mixed precision training\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Optimizing data loading\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "for epoch in range(5):\n",
    "    print(f\"Starting epoch {epoch + 1} for U-Net...\")\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (images, masks, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        #forward pass with mixed precision\n",
    "        with autocast():\n",
    "            outputs = unet_model(images)\n",
    "            loss = criterion_seg(outputs, masks)\n",
    "        \n",
    "        #backward pass\n",
    "        optimizer_seg.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer_seg)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        #print progress for every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{10}], Batch [{batch_idx + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "            \n",
    "print(f\"Epoch {epoch + 1} completed. Average Loss: {running_loss / len(dataloader):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h-dYVF_QGdpG",
    "outputId": "21c71f61-d55b-447d-8a79-f8c72a9b2206"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval().to(device)  # Ensure the model is on the right device\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            if len(batch) == 3:\n",
    "                images, masks, labels = batch  # Unpack images, masks, labels\n",
    "            elif len(batch) == 2:\n",
    "                images, labels = batch  # For cases with only images and labels\n",
    "\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()  # Convert to probabilities\n",
    "\n",
    "            # Convert probabilities to binary predictions (thresholding at 0.5)\n",
    "            binary_preds = (preds > 0.5).astype(int).flatten()\n",
    "\n",
    "            # Collect predictions and true labels\n",
    "            all_preds.extend(binary_preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Compute accuracy\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Run the accuracy evaluation\n",
    "evaluate_accuracy(efficientnet_model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DpyBBN6vUYBl",
    "outputId": "6fcbfaef-c67e-4268-b3a8-0256bc6352f8"
   },
   "outputs": [],
   "source": [
    "refuge_json_path = os.path.join(path, 'REFUGE', 'val', 'index.json')\n",
    "image_dir_refuge = os.path.join(path, 'REFUGE', 'val', 'Images_Cropped')\n",
    "mask_dir_refuge = os.path.join(path, 'REFUGE', 'val', 'Masks_Cropped')\n",
    "\n",
    "# Load the JSON file containing the actual values\n",
    "with open(refuge_json_path) as f:\n",
    "    refuge_data = json.load(f)\n",
    "\n",
    "# Prepare image paths, mask paths, and labels\n",
    "image_paths_refuge = []\n",
    "mask_paths_refuge = []\n",
    "labels_refuge = []\n",
    "\n",
    "for key in refuge_data:\n",
    "    img_info = refuge_data[key]\n",
    "    img_name = img_info['ImgName']\n",
    "    label = img_info['Label']\n",
    "\n",
    "    # Construct the full paths\n",
    "    image_paths_refuge.append(os.path.join(image_dir_refuge, img_name))\n",
    "    mask_paths_refuge.append(os.path.join(mask_dir_refuge, img_name.replace('.jpg', '.png')))  # Assuming masks are PNGs\n",
    "    labels_refuge.append(label)\n",
    "\n",
    "# Convert labels to a numpy array\n",
    "labels_refuge = np.array(labels_refuge)\n",
    "\n",
    "# Create a DataLoader for the REFUGE dataset\n",
    "refuge_dataset = GlaucomaDataset(image_paths_refuge, mask_paths_refuge, labels_refuge, transform=image_transform, mask_transform=mask_transform)\n",
    "refuge_dataloader = DataLoader(refuge_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "efficientnet_model.to(device)\n",
    "\n",
    "# Evaluate the REFUGE dataset\n",
    "evaluate_accuracy(efficientnet_model, refuge_dataloader)\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, masks, labels in refuge_dataloader:\n",
    "    images = images.to(device)\n",
    "    outputs = efficientnet_model(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_true, y_pred, target_names=[\"No Glaucoma\", \"Glaucoma\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "1jRyM2OaGdpH",
    "outputId": "9d16b8bd-a8f5-4020-f669-08176d50bf95"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_cdr(disc_mask, cup_mask):\n",
    "\n",
    "    disc_contours, _ = cv2.findContours(disc_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(disc_contours) > 0:\n",
    "        disc_cnt = max(disc_contours, key=cv2.contourArea)\n",
    "        _, _, _, disc_height = cv2.boundingRect(disc_cnt)\n",
    "    else:\n",
    "        disc_height = 1  # Avoid division by zero\n",
    "\n",
    "    cup_contours, _ = cv2.findContours(cup_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cup_contours) > 0:\n",
    "        cup_cnt = max(cup_contours, key=cv2.contourArea)\n",
    "        _, _, _, cup_height = cv2.boundingRect(cup_cnt)\n",
    "    else:\n",
    "        cup_height = 0\n",
    "\n",
    "    cdr = cup_height / disc_height\n",
    "    return cdr\n",
    "unet_model.eval()\n",
    "cdr_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        start_time = time.time()\n",
    "        output_masks = unet_model(images)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            output_mask = output_masks[i].cpu().numpy().squeeze()\n",
    "            output_mask = (output_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "            # Assume disc_mask and cup_mask separation logic here\n",
    "            disc_mask = output_mask  # Modify this based on your actual model output\n",
    "            cup_mask = output_mask    # Modify this based on your actual model output\n",
    "\n",
    "            # Calculate CDR\n",
    "            cdr = calculate_cdr(disc_mask, cup_mask)\n",
    "            cdr_list.append(cdr)\n",
    "\n",
    "print(cdr_list)  # Output the CDRs for verification\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
