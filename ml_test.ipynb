{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (2.5.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (0.20.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: albumentations in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (1.4.18)\n",
      "Requirement already satisfied: efficientnet_pytorch in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (0.7.1)\n",
      "Requirement already satisfied: kagglehub in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (1.14.1)\n",
      "Requirement already satisfied: scikit-image>=0.21.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (0.24.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.7.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (2.9.2)\n",
      "Requirement already satisfied: albucore==0.0.17 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (0.0.17)\n",
      "Requirement already satisfied: eval-type-backport in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (0.2.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: packaging in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from pydantic>=2.7.0->albumentations) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (2.36.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (2024.9.20)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from scikit-image>=0.21.0->albumentations) (0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from requests->kagglehub) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from requests->kagglehub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from requests->kagglehub) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\tonyz\\onedrive - the pennsylvania state university\\ds 440\\glaucomaproject\\.venv\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# installing the necessary environment variables\n",
    "%pip install torch torchvision opencv-python albumentations efficientnet_pytorch kagglehub pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tonyz\\OneDrive - The Pennsylvania State University\\DS 440\\GlaucomaProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 256, 256]) torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# data preparation \n",
    "# pytorch dataset for lunding the G1020 data\n",
    "\n",
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Read the image using OpenCV\n",
    "        image = cv2.imread(self.image_paths[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Corrected the typo here\n",
    "\n",
    "        # Apply any transformations if specified\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Get the label and convert it to tensor\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert from OpenCV image to PIL image\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "# Step 1: Download the dataset using kagglehub\n",
    "path = kagglehub.dataset_download(\"arnavjain1/glaucoma-datasets\")\n",
    "\n",
    "# Step 2: Set up paths and load CSV\n",
    "image_dir = os.path.join(path, 'G1020', 'Images')  # Adjust this to the correct image directory\n",
    "csv_file = os.path.join(path, 'G1020', 'G1020.csv')\n",
    "\n",
    "# Load the CSV file containing image names and labels\n",
    "df = pd.read_csv(csv_file)\n",
    "image_paths = [os.path.join(image_dir, img_name) for img_name in df['imageID']] \n",
    "labels = df['binaryLabels'].values \n",
    "\n",
    "# Step 3: Create the dataset and dataloader\n",
    "dataset = GlaucomaDataset(image_paths, labels, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Testing: Load a batch of images\n",
    "images, labels = next(iter(dataloader))\n",
    "print(images.shape, labels.shape)  # Should print torch.Size([16, 3, 256, 256]) and torch.Size([16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building UNet model (deep learning) for segmentation of the images\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # encoding\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "\n",
    "        # decoding\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec4 = self.conv_block(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec3 = self.conv_block(256, 128)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec2 = self.conv_block(128, 64)\n",
    "        self.conv_last = nn.Conv2d(64, 1, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # encoding\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(nn.MaxPool2d(2)(e1))\n",
    "        e3 = self.enc3(nn.MaxPool2d(2)(e2))\n",
    "        e4 = self.enc4(nn.MaxPool2d(2)(e3))\n",
    "\n",
    "        # decoding\n",
    "        d4 = self.upconv4(e4)\n",
    "        d4 = torch.cat((d4, e3), dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        d3 = self.upconv3(d4)\n",
    "        d3 = torch.cat((d3, e2), dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        d2 = self.upconv2(d3)\n",
    "        d2 = torch.cat((d2, e1), dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        return torch.sigmoid(self.conv_last(d2))\n",
    "\n",
    "unet_model = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "efficientnet_model = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "num_features = efficientnet_model._fc.in_features\n",
    "efficientnet_model._fc = nn.Linear(num_features, 1)  # Binary classification (glaucoma vs non-glaucoma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1 for U-Net...\n",
      "Epoch [1/10], Batch [10/64], Loss: 0.6931\n",
      "Epoch [1/10], Batch [20/64], Loss: 0.6931\n",
      "Epoch [1/10], Batch [30/64], Loss: 0.6931\n",
      "Epoch [1/10], Batch [40/64], Loss: 0.6931\n",
      "Epoch [1/10], Batch [50/64], Loss: 0.6931\n",
      "Epoch [1/10], Batch [60/64], Loss: 0.6931\n",
      "Epoch 1 completed. Average Loss: 0.6969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop for UNet (Segmentation)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "unet_model = unet_model.to(device)\n",
    "criterion_seg = nn.BCEWithLogitsLoss()\n",
    "optimizer_seg = optim.Adam(unet_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(1):\n",
    "    unet_model.train()\n",
    "    running_loss = 0.0\n",
    "    print(f\"Starting epoch {epoch + 1} for U-Net...\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        # Reshape labels\n",
    "        labels = labels.view(-1, 1, 1, 1)  # Reshape labels to [batch_size, 1, 1, 1]\n",
    "        labels = labels.expand(-1, -1, 256, 256)  # Expand to [batch_size, 1, 256, 256]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = unet_model(images)\n",
    "        loss = criterion_seg(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer_seg.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_seg.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/10], Batch [{batch_idx + 1}/{len(dataloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch + 1} completed. Average Loss: {running_loss / len(dataloader):.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC-ROC: 0.49293993579214573\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "def evaluate_classification_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    print(f\"AUC-ROC: {auc}\")\n",
    "\n",
    "evaluate_classification_model(efficientnet_model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_cdr(disc_mask, cup_mask):\n",
    "\n",
    "    disc_contours, _ = cv2.findContours(disc_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(disc_contours) > 0:\n",
    "        disc_cnt = max(disc_contours, key=cv2.contourArea)\n",
    "        _, _, _, disc_height = cv2.boundingRect(disc_cnt)\n",
    "    else:\n",
    "        disc_height = 1  # Avoid division by zero\n",
    "\n",
    "    cup_contours, _ = cv2.findContours(cup_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if len(cup_contours) > 0:\n",
    "        cup_cnt = max(cup_contours, key=cv2.contourArea)\n",
    "        _, _, _, cup_height = cv2.boundingRect(cup_cnt)\n",
    "    else:\n",
    "        cup_height = 0\n",
    "\n",
    "    cdr = cup_height / disc_height\n",
    "    return cdr\n",
    "unet_model.eval()\n",
    "cdr_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, _ in dataloader:\n",
    "        images = images.to(device)\n",
    "        start_time = time.time()\n",
    "        output_masks = unet_model(images)\n",
    "        inference_time = time.time() - start_time\n",
    "\n",
    "        for i in range(images.size(0)):\n",
    "            output_mask = output_masks[i].cpu().numpy().squeeze()\n",
    "            output_mask = (output_mask > 0.5).astype(np.uint8)\n",
    "\n",
    "            # Assume disc_mask and cup_mask separation logic here\n",
    "            disc_mask = output_mask  # Modify this based on your actual model output\n",
    "            cup_mask = output_mask    # Modify this based on your actual model output\n",
    "\n",
    "            # Calculate CDR\n",
    "            cdr = calculate_cdr(disc_mask, cup_mask)\n",
    "            cdr_list.append(cdr)\n",
    "\n",
    "print(cdr_list)  # Output the CDRs for verification\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
